{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLServer Quick-Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will help you get started creating machine learning microservices with MLServer \n",
    "in about 10 minutes. Our use case is to create a service that helps us compare the similarity \n",
    "between two documents. Think about whenever you are comparing which book, news article, blog post, \n",
    "tutorial (not to sound meta) to read next, wouldn't it be great to have a way to compare with \n",
    "similar ones that you have already likes (without having to rely on a recommendation's system)? \n",
    "That's what we'll focus on this tutorial, a document similarity service. ðŸ“œ + ðŸ“ƒ = ðŸ˜ŽðŸ‘ŒðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to install `mlserver`, the `spacy` library, and the language model `spacy` will need \n",
    "for our use case. We will also download the Wikipedia API library to test our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlserver spacy wikipedia-api\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the two commands above can be run in a notebook, hence the exclamation mark `!` at the beginning. If you \n",
    "are working from the command line, make sure you remove the `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](../images/mlserver_setup.png)\n",
    "\n",
    "At its core, MLServer requires that users give it 3 things, a `model-settings.json` file with \n",
    "information about the model, an (optional) `settings.json` file with information related to the server you \n",
    "are about to set up, and a `.py` file with the load-predict recipe for your model (as shown in the \n",
    "picture above). At a later step, whenever you are ready to package all of the components of your \n",
    "server into a docker image, you will also need to provide it with a `requirements.txt` file with \n",
    "containing dependencies of your server, but it is not necessary to have one to test your server \n",
    "locally. We'll get there in a few minutes.\n",
    "\n",
    "Let's create a directory for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_hub/quick-start/similarity_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a service that allows us to compare the similarity between two documents (our use case \n",
    "for this tutorial), it is good practice to test that our solution works first, especially if (like in with \n",
    "our use case) we're using a pre-trained model and/or pipeline. To test our use case, we'll be using \n",
    "[`spacy`](https://spacy.io/), a natural language processing library built with production in mind, and \n",
    "then we'll move on to building a microservice with MLServer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model loaded, let's look at the similarity of the abstracts of [Barbieheimer](https://en.wikipedia.org/wiki/Barbenheimer) \n",
    "using the Wikipedia API to see how similar these two movies actually are.\n",
    "\n",
    "To do this, we will be using the Wikipedia API Python library to find the summary for each \n",
    "of the movies. The main requirement of the API is that we pass in to main class, `Wikipedia()`, \n",
    "a project name, an email and the language we want information to be returned in. After that, \n",
    "we can search the for the movie summaries we want by passing the title of the movie to the \n",
    "`.page()` method and accessing the summary part with the `.summary` attribute.\n",
    "\n",
    "Feel free to change the movies for other documents or topics you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('MyMovieEval (example@example.com)', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barbie = wiki_wiki.page('Barbie_(film)').summary\n",
    "oppenheimer = wiki_wiki.page('Oppenheimer_(film)').summary\n",
    "\n",
    "print(barbie)\n",
    "print()\n",
    "print(oppenheimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our two summaries, let's compare them using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(barbie)\n",
    "doc2 = nlp(oppenheimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both summaries have information about the other movie, about \"films\" in general, \n",
    "and about the dates each aired (which is the same). The reality is that the model hasn't seen \n",
    "any of these movies so it might be generalizing to the context of each article, \"movies,\" \n",
    "rather than their content, \"dolls as humans and the atomic bomb.\"\n",
    "\n",
    "You should play around with different pages and see if what you get back is coherent with \n",
    "what you would expect.\n",
    "\n",
    "Time to create a machine learning API for our use-case. ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Building a Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Software as a Service (SaaS), a \"service\" refers to a software application \n",
    "or platform that is delivered over the internet, typically through a web browser or mobile \n",
    "app. The service provides a set of features and functionality that can be accessed by users \n",
    "on demand, and MLServer allows us to do that for machine learning models by leveraging the \n",
    "functionalities of libraries such as `asyncio`, `multiprocessing`, and `FastAPI`, among others. \n",
    "\n",
    "A \"client,\" on the other hand, refers to an You, Me, Us, individuals, or organization that \n",
    "use the SaaS service. Clients typically pay a subscription fee to access the service and use \n",
    "it for their own purposes, but we'll leave the commercial bit to you for after you complete \n",
    "this tutorial. ðŸ˜Ž\n",
    "\n",
    "To create a service with MLServer, we will define a class with two async functions, one that \n",
    "loads the data and another one to run inference (i.e. predict) with. The former will load the \n",
    "`spacy` model we tested in the last section, and the latter will take in a list with the two \n",
    "documents we want to compare. Lastly, our function will return a `numpy` array with a single \n",
    "value, our similarity score. We'll write the file to our `similarity_model` directory and call \n",
    "it `my_model.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/my_model.py\n",
    "\n",
    "from mlserver.codecs import decode_args\n",
    "from mlserver import MLModel\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class MyKulModel(MLModel):\n",
    "\n",
    "    async def load(self):\n",
    "        self.model = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    @decode_args\n",
    "    async def predict(self, docs: List[str]) -> np.ndarray:\n",
    "\n",
    "        doc1 = self.model(docs[0])\n",
    "        doc2 = self.model(docs[1])\n",
    "\n",
    "        return np.array(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model up and running, the last piece of the puzzle is to tell MLServer a bit of info \n",
    "about the model. In particular, it wants (or needs) to know the name of the model and how to implement \n",
    "it. The former can be anything you want (and it will be part of the URL of your API), and the latter will \n",
    "follow the recipe of `name_of_py_file_with_your_model.class_with_your_model`.\n",
    "\n",
    "Let's create the `model-settings.json` file MLServer is expecting inside our `similarity_model` directory \n",
    "and add the name and the implementation of our model to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/model-settings.json\n",
    "\n",
    "{\n",
    "    \"name\": \"doc-sim-model\",\n",
    "    \"implementation\": \"my_model.MyKulModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is in place, we can start serving predictions locally to test how things would play \n",
    "out for our future users. We'll initiate our server via the command line, and later on we'll see how to \n",
    "do the same via Python files. Here's where we are at right now in the process of developing microservices \n",
    "with MLServer.\n",
    "\n",
    "![start](../images/start_service.png)\n",
    "\n",
    "As you can see in the image, our server will be initialized with three entry points, one for HTTP requests, \n",
    "another for gRPC, and another for the metrics. To learn more about the powerful metrics feature of MLServer, \n",
    "please visit the relevant docs page [here](https://mlserver.readthedocs.io/en/latest/user-guide/metrics.html). \n",
    "To learn more about gRPC, please see this tutorial [here](https://realpython.com/python-microservices-grpc/).\n",
    "\n",
    "To start our service, open up a terminal and run the following command.\n",
    "\n",
    "```bash\n",
    "mlserver start models_hub/quick-start/similarity_model/\n",
    "```\n",
    "\n",
    "Note: If this is a fresh terminal, make sure you activate your environment before you run the command above. \n",
    "If you run the command above from your notebook (e.g. `!mlserver start ../models_hub/quick-start/similarity_model/`), \n",
    "you will have to send the request below from another notebook or terminal since the cell will continue to run \n",
    "until you turn it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Testing our Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to become a client of our service and test it. For this, we'll set up the payload we'll send \n",
    "to our service and use the `requests` library to [POST](https://www.baeldung.com/cs/http-get-vs-post) our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"docs\",\n",
    "          \"shape\": [2],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"parameters\": {\n",
    "              \"content_type\": \"str\"\n",
    "            },\n",
    "          \"data\": [barbie, oppenheimer]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://0.0.0.0:8080/v2/models/doc-sim-model/infer', json=inference_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our movies are {round(r.json()['outputs'][0]['data'][0] * 100, 4)}% similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose what just happened.\n",
    "\n",
    "The `URL` for our service might seem a bit odd if you've never heard of the V2/Open Inference Protocol (OIP). This \n",
    "protocol is a set of specifications that allows machine learning models to be shared and deployed in a \n",
    "standardized way. This protocol enables the use of machine learning models on a variety of platforms and \n",
    "devices without requiring changes to the model or its code. The OIP is useful because it allows us\n",
    "to integrate machine learning into a wide range of applications in a standard way.\n",
    "\n",
    "All URLs you create will MLServer will have the same structure.\n",
    "\n",
    "![v2](../images/urlv2.png)\n",
    "\n",
    "This kind of protocol is neither good nor bad but rather a standard to keep everyone on the same page. If you \n",
    "think about driving globally, your country has to apply a standard for driving on a particular side of the \n",
    "road, and this ensures everyone stays on the left (or the right depending on where you are at). Adopting this \n",
    "means that you won't have to wonder where the next driver is going to come out when you go out to run an errand, \n",
    "instead, you can focus on getting to where you're going to without much worrying.\n",
    "\n",
    "Let's describe what each of the components of our `inference_request` does.\n",
    "- `name`: this maps one-to-one to the name of the parameter in your `predict()` function.\n",
    "- `shape`: represents the shape of the elements in our `data`. In our case, it is a list with `[2]` strings.\n",
    "- `datatype`: the different data types expected by the server, e.g., str, numpy array, pandas dataframe, bytes, etc.\n",
    "- `parameters`: allows us to specify the `content_type` beyond the data types \n",
    "- `data`: the inputs to our predict function. These will be passed on automatically to the parameter \n",
    "when we use the `@decode_args` decorator on top of our `.predict()` function.\n",
    "\n",
    "To learn more about the OIP and how MLServer content types work, please have a looks at their \n",
    "[docs page here](https://mlserver.readthedocs.io/en/latest/user-guide/content-type.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Creating Model Replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you need to meet the demand of a high number of users and one model might not be enough, or is not using \n",
    "all of the resources of the instance it was allocated on. What we can do in this case is to create multiple \n",
    "replicas of our model to increase the throughput of the requests that come in. This can be particularly useful \n",
    "at the peak times of our server. To do this we need to tweak the Settings of our server via the `settings.json` \n",
    "file. In it, we'll add the number of independent model we want to have to the parameter `\"parallel_workers\": 3`.\n",
    "\n",
    "Let's stop our server, change the settings of it, start it again, and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/settings.json\n",
    "\n",
    "{\n",
    "    \"parallel_workers\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiplemodels](../images/multiple_models.png)\n",
    "\n",
    "As you can see in the output of the terminal, we now have 3 models running in parallel. The reason you might see 4 \n",
    "is because, by default, MLServer will print the name of the initialized model if it is one or more, and it will also \n",
    "print one for each model replica specified in the settings.\n",
    "\n",
    "Let's get a few more [twin films examples](https://en.wikipedia.org/wiki/Twin_films) to test our server. Get \n",
    "as creative as you'd like. ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_impact    = wiki_wiki.page('Deep_Impact_(film)').summary\n",
    "armageddon     = wiki_wiki.page('Armageddon_(1998_film)').summary\n",
    "\n",
    "antz           = wiki_wiki.page('Antz').summary\n",
    "a_bugs_life    = wiki_wiki.page(\"A_Bug's_Life\").summary\n",
    "\n",
    "the_dark_night = wiki_wiki.page('The_Dark_Knight').summary\n",
    "mamma_mia      = wiki_wiki.page('Mamma_Mia!_(film)').summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_score(movie1, movie2):\n",
    "    response = requests.post(\n",
    "        'http://0.0.0.0:8080/v2/models/doc-sim-model/infer', \n",
    "        json={\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                \"name\": \"docs\",\n",
    "                \"shape\": [2],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"parameters\": {\n",
    "                    \"content_type\": \"str\"\n",
    "                    },\n",
    "                \"data\": [movie1, movie2]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return response.json()['outputs'][0]['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test that the function works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim_score(deep_impact, armageddon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's map three POST requests at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(\n",
    "    map(get_sim_score, (deep_impact, antz, the_dark_night), (armageddon, a_bugs_life, mamma_mia))\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie1, movie2 in zip((deep_impact, antz, the_dark_night), (armageddon, a_bugs_life, mamma_mia)):\n",
    "    print(get_sim_score(movie1, movie2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Packaging our Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![serving3](../images/serving_2.png)\n",
    "\n",
    "For the last step of this quick start guide, we are going to package our model and service into a \n",
    "docker image that we can reuse in another project, or share it with colleagues immediately. This step \n",
    "requires that we have docker installed and configured in our PCs, so if you need to set that up \n",
    "you can do so by following the documentation [here](https://docs.docker.com/get-docker/).\n",
    "\n",
    "The first step is to create a `requirements.txt` file with all of our dependencies and add it to \n",
    "the directory we've been using for our service (`similarity_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/requirements.txt\n",
    "\n",
    "mlserver\n",
    "spacy==3.6.0\n",
    "https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build a docker image with our model, its dependencies and our server. If you've never heard \n",
    "of docker images before, here's a short description.\n",
    "\n",
    "> A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including code, libraries, dependencies, and settings. It's like a carry-on bag for your application, containing everything it needs to travel safely and run smoothly in different environments. Just as a carry-on bag allows you to bring your essentials with you on a trip, a Docker image enables you to transport your application and its requirements across various computing environments, ensuring consistent and reliable deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver build ../models_hub/quick-start/similarity_model/ -t 'fancy_ml_service'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that our image was successfully build not only by looking at the logs of the previous \n",
    "command but also with the `docker images` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our image works as intended with the following command. Make sure you have closed your \n",
    "previous server by using `CTRL + C` in your terminal.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8080:8080 fancy_ml_service\n",
    "```\n",
    "\n",
    "Now that you have a packaged and fully-functioning microservice with our model, we could deploy it container \n",
    "via the diverse set of offerings available through different cloud providers (e.g. AWS Lambda, Google Cloud Run, \n",
    "ect.), on your company's Kubernetes cluster (if they have one up and running), or anywhere else where you \n",
    "can bring in a docker image with you to run in some virtual machine.\n",
    "\n",
    "To learn more about MLServer and the different ways in which you can use it, head over to the \n",
    "[examples](https://mlserver.readthedocs.io/en/latest/examples/index.html) section \n",
    "or the [user guide](https://mlserver.readthedocs.io/en/latest/user-guide/index.html). To learn about \n",
    "some of the deployment options available, head over to the docs [here](https://mlserver.readthedocs.io/en/stable/user-guide/deployment/index.html).\n",
    "\n",
    "To keep up to date with what we are up to at Seldon, make sure you join our \n",
    "[Slack community](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbundle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
