{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLServer Quick-Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will help you get started creating machine learning microservices with MLServer \n",
    "in about 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to install `mlserver`, the `spacy` library and the language model it will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlserver spacy wikipedia-api\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](../images/mlserver_setup.png)\n",
    "\n",
    "At its core, MLServer requires that users give it 3 things, a `model-settings.json` file with \n",
    "information about the model, a `settings.json` file with information related to the server you \n",
    "are about to set up, and a `.py` with the load-predict recipe for your model (as shown in the \n",
    "picture above). At a later step, whenever you are ready to package all of the components of your \n",
    "server, you will also need to provide it with a `requirements.txt` file with the dependencies \n",
    "of your server.\n",
    "\n",
    "Let's create a directory for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_hub/quick-start/similarity_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a service that allows us to compare the similarity between two documents. Before \n",
    "we do so, let's develop our use case using `spacy`, a natural language processing library built with \n",
    "production in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model loaded, let's look at the similarity of the abstracts of (Barbieheimer)[https://en.wikipedia.org/wiki/Barbenheimer] \n",
    "using the Wikipedia API to see how similar these two actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('MyMovieEval (example@example.com)', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barbie = wiki_wiki.page('Barbie_(film)').summary\n",
    "oppenheimer = wiki_wiki.page('Oppenheimer_(film)').summary\n",
    "\n",
    "print(barbie)\n",
    "print()\n",
    "print(oppenheimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our two summaries, let's compare them using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(barbie)\n",
    "doc2 = nlp(oppenheimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both summaries have information about the other movie, about \"films\" in general, \n",
    "and about the dates each aired (which is the same). The reality is that the model hasn't seen \n",
    "any of these movies so it might be generalizing to the context of each article, \"movies,\" \n",
    "rather than their content, \"dolls as humans and the atomic bomb.\"\n",
    "\n",
    "You should play around with different pages and see if what you get back is coherent with \n",
    "what you would expect.\n",
    "\n",
    "Time to create a machine learning API for our use-case. ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/my_model.py\n",
    "\n",
    "from mlserver.codecs import decode_args\n",
    "from mlserver import MLModel\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class MyKulModel(MLModel):\n",
    "\n",
    "    async def load(self):\n",
    "        self.model = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    @decode_args\n",
    "    async def predict(self, docs: List[str]) -> np.ndarray:\n",
    "\n",
    "        doc1 = self.model(docs[0])\n",
    "        doc2 = self.model(docs[1])\n",
    "\n",
    "        return np.array(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model up and running, the last piece of the puzzle is to tell MLServer a bit of info \n",
    "about the model. In particular, it wants to know its name, and how to implement it. The former can be anything \n",
    "you want (and it will be part of the URL of your API), and the latter will be `name_of_py_file.class_with_your_model`. Let's \n",
    "add this file to the directory with out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/model-settings.json\n",
    "\n",
    "{\n",
    "    \"name\": \"doc-sim-model\",\n",
    "    \"implementation\": \"my_model.MyKulModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is in place, we can start serving predictions locally to test how things would play \n",
    "out for our future users. We'll initiate our server via the command line, and later on we'll see how to \n",
    "do the same via Python files. Here's where we are at right now in the process of developing microservices \n",
    "with MLServer.\n",
    "\n",
    "![start](../images/start_service.png)\n",
    "\n",
    "Open up a terminal and run the following command.\n",
    "```bash\n",
    "mlserver start models_hub/quick-start/similarity_model/\n",
    "```\n",
    "\n",
    "Note: Make sure to activate your environment before you run the command above. If you run the command above \n",
    "from your notebook (e.g. `!mlserver start ../models_hub/quick-start/similarity_model/`), you will have to send \n",
    "the request below from another notebook or terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test our service. We'll use the `requests` library and set up the payload we'll send to our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"docs\",\n",
    "          \"shape\": [2],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"parameters\": {\n",
    "              \"content_type\": \"str\"\n",
    "            },\n",
    "          \"data\": [barbie, oppenheimer]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://0.0.0.0:8080/v2/models/doc-sim-model/infer', json=inference_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(r.json()['outputs'][0]['data'][0] * 100, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose what just happened.\n",
    "\n",
    "The `URL` for our service might seem a bit odd if you've never heard of the V2/Open Inference Protocol. This \n",
    "protocol is a set of specifications that allows machine learning models to be shared and deployed in a \n",
    "standardized way. This protocol enables the use of machine learning models on a variety of platforms and \n",
    "devices without requiring changes to the model or its code. The V2 protocol is useful because it allows for \n",
    "faster and more efficient deployment of machine learning models, making it easier to integrate machine \n",
    "learning into a wide range of applications.\n",
    "\n",
    "All URLs you create will MLServer will have the same structure.\n",
    "\n",
    "![v2](../images/urlv2.png)\n",
    "\n",
    "This kind of protocol is neither good nor bad but rather a standard to keep everyone on the same page. If you \n",
    "think about driving globally, your country has to apply a standard for driving on a particular side of the \n",
    "street, and this ensures everyone stays on the left (or the right depending on where you are at). What this does \n",
    "for you and the rest of the people around you, is that it saves you from having to wonder where the next driver \n",
    "is going to come out from and, instead, it lets's you focus getting to where you're going to without much worrying.\n",
    "\n",
    "Lastly, the `inference_request` we created follows the OIP, and as such, we need to build the request in a specific format.\n",
    "- `name`:\n",
    "- `shape`:\n",
    "- `datatype`:\n",
    "- `parameters`:\n",
    "- `data`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model Replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you need to meet the demand of a high number of users and one model is will not suffice, or is not using \n",
    "all of the resources of the instance it was allocated on. What we can do in this case is to create multiple \n",
    "replicas of our model to increase its throughput. To do so we need to turn to the `settings.json` file and \n",
    "add the number of independent model we want to have to the parameter `parallel_workers=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging our Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/serving_2.png)\n",
    "\n",
    "For the next step we are going to package our model and service into a docker image that we can reuse or\n",
    "share with colleagues. This step requires that you have docker installed and running in your PC, so if you \n",
    "need to set that up you can do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models_hub/quick-start/similarity_model/requirements.txt\n",
    "\n",
    "mlserver\n",
    "spacy==3.6.0\n",
    "https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver build ../models_hub/quick-start/similarity_model/ -t 'fancy_ml_service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our image works as intended with the following command. Make sure you have closed your \n",
    "previous server by using `CTRL + C` in your terminal.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8080:8080 fancy_ml_service\n",
    "```\n",
    "\n",
    "Now that you have a fully functioning set of microservices, you can deploy your container via the diverse \n",
    "set of offerings available through different cloud providers, on your company's Kubernetes cluster if they \n",
    "have one up and running, or anywhere else where you can bring in a docker image to.\n",
    "\n",
    "To learn more about MLServer and the different ways in which you can use it, head over to the examples section \n",
    "or the user guide, and to keep up to date with what we are up to at Seldon, make sure you join our Slack community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbundle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
